from typing import Literal, Optional, Union
from litellm.utils import trim_messages, get_max_tokens, token_counter
from litellm.litellm_core_utils.prompt_templates.common_utils import (
    get_completion_messages,
)
from litellm.integrations.custom_guardrail import CustomGuardrail
from litellm.proxy._types import UserAPIKeyAuth
from litellm.caching.caching import DualCache
import logging
import yaml
from pathlib import Path

logger = logging.getLogger(__name__)


class MessageTrimmingGuardrail(CustomGuardrail):
    def __init__(
        self,
        **kwargs,
    ):
        # store kwargs as optional_params
        self.optional_params = kwargs

        super().__init__(**kwargs)

        default_config = self._get_default_config(
            self.optional_params.get("guardrail_name")
        )

        # Default Configuration parameters
        self.trim_ratio = default_config.get("trim_ratio", 0.75)
        self.max_output_tokens = default_config.get("max_output_tokens", 2000)
        self.safety_buffer = default_config.get("safety_buffer", 500)
        self.debug = default_config.get("debug", False)

    def _load_config(self):
        with open("config.yaml", "r") as file:
            config = yaml.safe_load(file)
        return config

    def _get_default_config(self, guardrail_name: str):
        """Extract default_config for this guardrail from the full config."""
        try:
            config = self._load_config()
            guardrails = config.get("guardrails", [])
            for guardrail in guardrails:
                if guardrail.get("guardrail_name") == guardrail_name:
                    return guardrail.get("default_config", {})
            return {}
        except Exception as e:
            logger.warning(f"Failed to extract default_config: {e}")
            return {}

    def _log_debug(self, message: str):
        """Log debug messages if debug mode is enabled."""
        if self.debug:
            print(f"[GUARDRAIL] {message}")

    def _calculate_safe_completion_tokens(
        self,
        max_context_tokens: int,
        current_input_tokens: int,
        requested_completion: int,
    ) -> int:
        """Calculate safe completion tokens based on context and input constraints.

        Args:
            max_context_tokens: Maximum context window size for the model
            current_input_tokens: Current number of input tokens
            requested_completion: Requested number of completion tokens

        Returns:
            Safe number of completion tokens that won't exceed context window
        """
        available_for_completion = (
            max_context_tokens - int(current_input_tokens) - self.safety_buffer
        )
        safe_completion_tokens = min(
            requested_completion, max(256, int(available_for_completion * 0.75))
        )
        return safe_completion_tokens

    def _update_completion_tokens(
        self,
        data: dict,
        safe_completion_tokens: int,
        has_max_tokens: bool,
        has_max_completion: bool,
    ) -> None:
        """Update completion token limits in the request data.

        Args:
            data: The request data dictionary to update
            safe_completion_tokens: The safe number of completion tokens to set
            has_max_tokens: Whether max_tokens was originally in the data
            has_max_completion: Whether max_completion_tokens was originally in the data
        """
        if has_max_tokens:
            self._log_debug(
                f"Updating max_tokens from {data['max_tokens']} to {safe_completion_tokens}"
            )
            data["max_tokens"] = safe_completion_tokens
        elif has_max_completion:
            self._log_debug(
                f"Updating max_completion_tokens from {data['max_completion_tokens']} to {safe_completion_tokens}"
            )
            data["max_completion_tokens"] = safe_completion_tokens
        else:
            # If neither was set, we need to add one to prevent default from being too large
            self._log_debug(
                f"No completion token limit set, adding max_tokens={safe_completion_tokens}"
            )
            data["max_tokens"] = safe_completion_tokens

    async def async_pre_call_hook(
        self,
        user_api_key_dict: UserAPIKeyAuth,
        cache: DualCache,
        data: dict,
        call_type: Literal[
            "completion",
            "text_completion",
            "embeddings",
            "image_generation",
            "moderation",
            "audio_transcription",
            "pass_through_endpoint",
            "rerank",
        ],
    ) -> Optional[Union[Exception, str, dict]]:
        # Trim messages if present
        if "messages" in data and data["messages"]:
            model = data.get("model")

            # Get model's context window size
            try:
                max_context_tokens = get_max_tokens(model)
            except:
                max_context_tokens = 8192  # Default fallback

            self._log_debug(f"Model: {model}")
            self._log_debug(f"Max context tokens: {max_context_tokens}")

            # Get requested completion tokens
            requested_completion = (
                data.get("max_tokens")
                or data.get("max_completion_tokens")
                or self.max_output_tokens
            )

            self._log_debug(f"Requested completion tokens: {requested_completion}")

            # Count current input tokens
            try:
                current_tokens = token_counter(model=model, messages=data["messages"])
            except Exception as e:
                # Fallback: estimate tokens
                current_tokens = sum(
                    len(str(msg.get("content", "")).split()) * 1.3
                    for msg in data["messages"]
                )
                self._log_debug(f"Token counting failed ({e}), using estimate")

            self._log_debug(f"Current input tokens: {current_tokens}")

            # Calculate safe completion tokens FIRST based on current input
            # Use a larger safety margin to account for LiteLLM adding extra tokens
            safe_completion_tokens = self._calculate_safe_completion_tokens(
                max_context_tokens, current_tokens, requested_completion
            )

            self._log_debug(f"Safe completion tokens: {safe_completion_tokens}")
            self._log_debug(
                f"Calculation: min({requested_completion}, max(512, ({max_context_tokens} - {int(current_tokens)} - {self.safety_buffer}) * 0.90))"
            )

            # Update completion tokens in the request
            has_max_tokens = "max_tokens" in data
            has_max_completion = "max_completion_tokens" in data

            self._update_completion_tokens(
                data, safe_completion_tokens, has_max_tokens, has_max_completion
            )

            # Now calculate max input tokens based on the safe completion tokens
            # Be even more conservative here
            max_input_tokens = int(
                (max_context_tokens - safe_completion_tokens - self.safety_buffer)
                * 0.85
            )  # Add 85% multiplier for extra safety

            self._log_debug(f"Max input tokens allowed: {max_input_tokens}")

            # ALWAYS trim to be safe, accounting for tokens that will be added later
            if current_tokens > max_input_tokens:
                self._log_debug(
                    f"Input tokens ({current_tokens}) exceed limit ({max_input_tokens}), trimming messages..."
                )
                # Trim messages to fit
                data["messages"] = trim_messages(
                    data["messages"],
                    model=model,
                    max_tokens=int(
                        max_input_tokens * 0.90
                    ),  # Trim to 90% of already conservative max
                    trim_ratio=self.trim_ratio,
                )

                # Ensure "ensure_alternating_roles" is fixed for message after trim
                data["messages"] = get_completion_messages(
                    messages=data["messages"],
                    assistant_continue_message={"role": "assistant", "content": ""},
                    user_continue_message={
                        "role": "user",
                        "content": "Please continue",
                    },
                    ensure_alternating_roles=True,
                )
                # Recount after trimming
                try:
                    new_token_count = token_counter(
                        model=model, messages=data["messages"]
                    )
                    self._log_debug(f"After trimming, input tokens: {new_token_count}")

                    # RECALCULATE safe completion tokens based on actual trimmed input
                    safe_completion_tokens = self._calculate_safe_completion_tokens(
                        max_context_tokens, new_token_count, requested_completion
                    )
                    self._log_debug(
                        f"Recalculated safe completion tokens after trim: {safe_completion_tokens}"
                    )

                    # Update the data with the recalculated values
                    self._update_completion_tokens(
                        data, safe_completion_tokens, has_max_tokens, has_max_completion
                    )

                    # Update for final logging
                    current_tokens = new_token_count
                except Exception as e:
                    self._log_debug(f"Failed to recount tokens after trim: {e}")
            else:
                self._log_debug(
                    f"No trimming needed, but current={current_tokens}, max={max_input_tokens}"
                )

            self._log_debug(
                f"Expected total: input ~{int(current_tokens)} + completion {safe_completion_tokens} + buffer {self.safety_buffer} = ~{int(current_tokens) + safe_completion_tokens + self.safety_buffer}/{max_context_tokens}"
            )

        return data
