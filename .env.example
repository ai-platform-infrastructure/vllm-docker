COMPOSE_PROJECT_NAME=vllm_api
COMPOSE_DOMAIN=vllmapi.local.itkdev.dk
COMPOSE_LITELLM_DOMAIN=litellm.local.itkdev.dk

HUGGING_FACE_HUB_TOKEN=xxx

# Qwen3-Next-80B MoE with NVIDIA FP4 quantization for Blackwell RTX PRO 6000 (96GB VRAM)
# 64k context, high memory utilization
VLLM_MODEL=nvidia/Qwen3-Next-80B-A3B-Instruct-NVFP4
VLLM_OPTIONS='--dtype auto --max-model-len 131072 --gpu-memory-utilization 0.92 --tensor-parallel-size 1 --max-num-batched-tokens 32768 --max-num-seqs 512 --num-scheduler-steps 15 --enable-prefix-caching --kv-cache-dtype fp8 --disable-log-requests --attention-backend FLASHINFER --tool-call-parser hermes --enable-auto-tool-choice'
SHM_SIZE=32gb

# Blackwell-optimized FP4 MoE kernels
VLLM_USE_FLASHINFER_MOE_FP4=1

# Leave empty if you do not wish to use API key.
VLLM_API_KEY=

POSTGRES_PASSWORD=

LITELLM_MASTER_KEY=
LITELLM_SALT_KEY=

# Speculative decoding using Qwen3-Next's native multi-token prediction.
# Reduces latency without quality degradation. Tune num_speculative_tokens (1-5).
# Append to VLLM_OPTIONS as: --speculative-config '{"method": "qwen3_next_mtp", "num_speculative_tokens": 2}'